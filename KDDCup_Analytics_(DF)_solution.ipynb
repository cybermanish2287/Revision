{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03g8beoOPBMh"
      },
      "source": [
        "**KDDCup Data Analytics with PySpark DF: A structured case study**<a href=\"#KDDCup-Data-Analytics-with-PySpark-DF:-A-structured-case-study\" class=\"anchor-link\">¶</a>\n",
        "=====================================================================================================================================================================\n",
        "\n",
        "### Udemy Course: Best Hands-on Big Data Practices and Use Cases using PySpark<a href=\"#Udemy-Course:-Best-Hands-on-Big-Data-Practices-and-Use-Cases-using-PySpark\" class=\"anchor-link\">¶</a>\n",
        "\n",
        "### Author: Amin Karami (PhD, FHEA)<a href=\"#Author:-Amin-Karami-(PhD,-FHEA)\" class=\"anchor-link\">¶</a>\n",
        "\n",
        "##### data source: <http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html><a href=\"#data-source:-http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\" class=\"anchor-link\">¶</a>\n",
        "\n",
        "In \\[ \\]:\n",
        "\n",
        "    ########## ONLY in Colab ##########\n",
        "    !pip3 install pyspark\n",
        "    ########## ONLY in Colab ##########\n",
        "\n",
        "In \\[ \\]:\n",
        "\n",
        "    ########## ONLY in Ubuntu Machine ##########\n",
        "    # Load Spark engine\n",
        "    !pip3 install -q findspark\n",
        "    import findspark\n",
        "    findspark.init()\n",
        "    ########## ONLY in Ubuntu Machine ##########\n",
        "\n",
        "In \\[ \\]:\n",
        "\n",
        "    # import SparkSession\n",
        "\n",
        "    from pyspark.sql import SparkSession\n",
        "    spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "    spark\n",
        "\n",
        "In \\[ \\]:\n",
        "\n",
        "    # Read and Load Data to Spark\n",
        "    # Data source: http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
        "\n",
        "    df = spark.read.text(\"kddcup.data.gz\")\n",
        "    df.printSchema()\n",
        "\n",
        "In \\[ \\]:\n",
        "\n",
        "    # Print data\n",
        "    df.show()\n",
        "\n",
        "In \\[ \\]:\n",
        "\n",
        "    # Split data (if needed)\n",
        "\n",
        "    from pyspark.sql.functions import split\n",
        "\n",
        "    split_col = split(df['value'], ',')\n",
        "    df = df.withColumn('Protocol', split_col.getItem(1)) \\\n",
        "           .withColumn('Service', split_col.getItem(2)) \\\n",
        "           .withColumn('flag', split_col.getItem(3)) \\\n",
        "           .withColumn('src_bytes', split_col.getItem(4)) \\\n",
        "           .withColumn('dst_bytes', split_col.getItem(5)) \\\n",
        "           .withColumn('urgent', split_col.getItem(8)) \\\n",
        "           .withColumn('num_failed_logins', split_col.getItem(10)) \\\n",
        "           .withColumn('root_shell', split_col.getItem(13)) \\\n",
        "           .withColumn('guest_login', split_col.getItem(21)) \\\n",
        "           .withColumn('label', split_col.getItem(41)) \\\n",
        "           .drop('value')\n",
        "\n",
        "    df.show()\n",
        "\n",
        "    +--------+-------+----+---------+---------+------+-----------------+----------+-----------+-------+\n",
        "    |Protocol|Service|flag|src_bytes|dst_bytes|urgent|num_failed_logins|root_shell|guest_login|  label|\n",
        "    +--------+-------+----+---------+---------+------+-----------------+----------+-----------+-------+\n",
        "    |     tcp|   http|  SF|      215|    45076|     0|                0|         0|          0|normal.|\n",
        "    |     tcp|   http|  SF|      162|     4528|     0|                0|         0|          0|normal.|\n",
        "    |     tcp|   http|  SF|      236|     1228|     0|                0|         0|          0|normal.|\n",
        "    |     tcp|   http|  SF|      233|     2032|     0|                0|         0|          0|normal.|\n",
        "    |     tcp|   http|  SF|      239|      486|     0|                0|         0|          0|normal.|\n",
        "    |     tcp|   http|  SF|      238|     1282|     0|                0|         0|          0|normal.|\n",
        "    |     tcp|   http|  SF|      235|     1337|     0|                0|         0|          0|normal.|\n",
        "    |     tcp|   http|  SF|      234|     1364|     0|                0|         0|          0|normal.|\n",
        "    |     tcp|   http|  SF|      239|     1295|     0|                0|         0|          0|normal.|\n",
        "    |     tcp|   http|  SF|      181|     5450|     0|                0|         0|          0|normal.|\n",
        "    |     tcp|   http|  SF|      184|      124|     0|                0|         0|          0|normal.|\n",
        "    |     tcp|   http|  SF|      185|     9020|     0|                0|         0|          0|normal.|\n",
        "    |     tcp|   http|  SF|      239|     1295|     0|                0|         0|          0|normal.|\n",
        "    |     tcp|   http|  SF|      181|     5450|     0|                0|         0|          0|normal.|\n",
        "    |     tcp|   http|  SF|      236|     1228|     0|                0|         0|          0|normal.|\n",
        "    |     tcp|   http|  SF|      233|     2032|     0|                0|         0|          0|normal.|\n",
        "    |     tcp|   http|  SF|      238|     1282|     0|                0|         0|          0|normal.|\n",
        "    |     tcp|   http|  SF|      235|     1337|     0|                0|         0|          0|normal.|\n",
        "    |     tcp|   http|  SF|      234|     1364|     0|                0|         0|          0|normal.|\n",
        "    |     tcp|   http|  SF|      239|      486|     0|                0|         0|          0|normal.|\n",
        "    +--------+-------+----+---------+---------+------+-----------------+----------+-----------+-------+\n",
        "    only showing top 20 rows\n",
        "\n",
        "In \\[ \\]:\n",
        "\n",
        "    # Increase the number of partitions (if needed) and Build a Temp table\n",
        "\n",
        "    df = df.repartition(10)\n",
        "    print(df.rdd.getNumPartitions())\n",
        "\n",
        "    df.createOrReplaceTempView(\"df_KDDCup\")\n",
        "\n",
        "    10\n",
        "\n",
        "Question 1: Count the number of connections for each label<a href=\"#Question-1:-Count-the-number-of-connections-for-each-label\" class=\"anchor-link\">¶</a>\n",
        "---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "In \\[ \\]:\n",
        "\n",
        "    df.groupBy('label').count().orderBy('count', ascending= False).show()\n",
        "\n",
        "Question 2: Get the list of `Protocols`that are `normal` and `vulnerable to attacks`, where there is NOT `guest login` to the destination addresses.<a href=\"#Question-2:--Get-the-list-of-Protocolsthat-are-normal-and-vulnerable-to-attacks,-where-there-is-NOT-guest-login-to-the-destination-addresses.\" class=\"anchor-link\">¶</a>\n",
        "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "In \\[ \\]:\n",
        "\n",
        "    sql_query = \"\"\" SELECT Protocol,\n",
        "                    CASE label\n",
        "                      WHEN 'normal.' THEN 'no attack'\n",
        "                      ELSE 'attack'\n",
        "                    END AS State,\n",
        "                    COUNT(*) as freq\n",
        "                  FROM df_KDDCup\n",
        "                  WHERE guest_login != 1\n",
        "                  GROUP BY Protocol, State\n",
        "                  ORDER BY Protocol DESC\n",
        "                \"\"\"\n",
        "\n",
        "    spark.sql(sql_query).show()\n",
        "\n",
        "    +--------+---------+-------+\n",
        "    |Protocol|    State|   freq|\n",
        "    +--------+---------+-------+\n",
        "    |     udp|no attack| 191348|\n",
        "    |     udp|   attack|   2940|\n",
        "    |     tcp|no attack| 764894|\n",
        "    |     tcp|   attack|1101613|\n",
        "    |    icmp|   attack|2820782|\n",
        "    |    icmp|no attack|  12763|\n",
        "    +--------+---------+-------+\n",
        "\n",
        "Question 3: Apply Some Descriptive Statistics on Numerical Data<a href=\"#Question-3:-Apply-Some-Descriptive-Statistics-on-Numerical-Data\" class=\"anchor-link\">¶</a>\n",
        "-------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "In \\[ \\]:\n",
        "\n",
        "    # PySpark provides built-in standard Aggregate functions defines in DataFrame API\n",
        "    from pyspark.sql.functions import *\n",
        "\n",
        "    summary = df.select(mean(df.src_bytes).alias('AVG(src_byte)'),\n",
        "                        stddev(df.src_bytes).alias('STD(src_byte)'),\n",
        "                        min(df.src_bytes).alias('MIN(src_byte)'),\n",
        "                        max(df.src_bytes).alias('MAX(src_byte)'),\n",
        "                        last(df.src_bytes).alias('LAST(src_byte)'),\n",
        "                        skewness(df.src_bytes).alias('SKEW(src_byte)'),\n",
        "                      )\n",
        "    summary.show()\n",
        "\n",
        "    +------------------+-----------------+-------------+-------------+--------------+------------------+\n",
        "    |     AVG(src_byte)|    STD(src_byte)|MIN(src_byte)|MAX(src_byte)|LAST(src_byte)|    SKEW(src_byte)|\n",
        "    +------------------+-----------------+-------------+-------------+--------------+------------------+\n",
        "    |1834.6211752293746|941431.0744911298|            0|          999|          2341|1188.9519100465736|\n",
        "    +------------------+-----------------+-------------+-------------+--------------+------------------+\n",
        "\n",
        "In \\[ \\]:\n",
        "\n",
        "    groups = df.groupBy(\"Protocol\")\n",
        "    groups.agg({'src_bytes':'mean', 'dst_bytes':'stddev'}).show()\n",
        "\n",
        "    +--------+-----------------+------------------+\n",
        "    |Protocol|   avg(src_bytes)| stddev(dst_bytes)|\n",
        "    +--------+-----------------+------------------+\n",
        "    |     tcp|3388.569965326596|1043771.3100418178|\n",
        "    |     udp|97.22772893848308| 55.43318653434132|\n",
        "    |    icmp|927.8916893855577|               0.0|\n",
        "    +--------+-----------------+------------------+\n",
        "\n",
        "Question 4: A descriptive stats based on `Protocols` and `Labels`<a href=\"#Question-4:-A-descriptive-stats-based-on-Protocols-and-Labels\" class=\"anchor-link\">¶</a>\n",
        "-------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "In \\[ \\]:\n",
        "\n",
        "    sql_query = \"\"\"\n",
        "                               SELECT protocol,\n",
        "                                 CASE label\n",
        "                                   WHEN 'normal.' THEN 'no attack'\n",
        "                                   ELSE 'attack'\n",
        "                                 END AS state,\n",
        "                                 COUNT(*) as total_freq,\n",
        "                                ROUND(AVG(src_bytes), 2) as mean_src_bytes,\n",
        "                                ROUND(AVG(dst_bytes), 2) as mean_dst_bytes,\n",
        "                                SUM(urgent) as sum_urgent,\n",
        "                                SUM(num_failed_logins) as sum_failed_logins,\n",
        "                                SUM(root_shell) as sum_root_shell,\n",
        "                                SUM(guest_login) as sum_guest_login\n",
        "                               FROM df_KDDCup\n",
        "                               GROUP BY protocol, state\n",
        "                               ORDER BY 3 DESC\n",
        "                               \"\"\"\n",
        "    spark.sql(sql_query).show()\n",
        "\n",
        "    +--------+---------+----------+--------------+--------------+----------+-----------------+--------------+---------------+\n",
        "    |protocol|    state|total_freq|mean_src_bytes|mean_dst_bytes|sum_urgent|sum_failed_logins|sum_root_shell|sum_guest_login|\n",
        "    +--------+---------+----------+--------------+--------------+----------+-----------------+--------------+---------------+\n",
        "    |    icmp|   attack|   2820782|        931.68|           0.0|       0.0|              0.0|           0.0|            0.0|\n",
        "    |     tcp|   attack|   1101928|       4465.81|       2005.96|       4.0|             61.0|          32.0|          315.0|\n",
        "    |     tcp|no attack|    768670|       1844.29|       4071.32|      35.0|             96.0|         302.0|         3776.0|\n",
        "    |     udp|no attack|    191348|         98.32|         89.41|       0.0|              0.0|           0.0|            0.0|\n",
        "    |    icmp|no attack|     12763|         90.68|           0.0|       0.0|              0.0|           0.0|            0.0|\n",
        "    |     udp|   attack|      2940|          26.4|          0.82|       0.0|              0.0|           0.0|            0.0|\n",
        "    +--------+---------+----------+--------------+--------------+----------+-----------------+--------------+---------------+\n",
        "\n",
        "Question 5: Get the frquency of `services` for the original `UDP and ICMP` based `attacks`<a href=\"#Question-5:-Get-the-frquency-of-services-for-the-original-UDP-and-ICMP-based-attacks\" class=\"anchor-link\">¶</a>\n",
        "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "(hint 1: original attacks: `[dos, u2r, r2l, and probe]`)\n",
        "\n",
        "(hint 2: returns the `services` and `protocols` center justified)\n",
        "\n",
        "In \\[ \\]:\n",
        "\n",
        "    from pyspark.sql.types import StringType\n",
        "\n",
        "    def Attack_Category(item):\n",
        "      if item.replace(\".\", \"\") in ['back', 'land', 'neptune', 'pod', 'smurf', 'teardrop']:\n",
        "        return \"DoS\"\n",
        "      elif item.replace(\".\", \"\") in ['buffer_overflow', 'loadmodule', 'perl', 'rootkit']:\n",
        "        return \"U2R\"\n",
        "      elif item.replace(\".\", \"\") in ['ftp_write', 'guess_passwd', 'multihop', 'phf', 'spy', 'warezclient', 'warezmaster']:\n",
        "        return \"R2L\"\n",
        "      else: return \"Probe\" \n",
        "\n",
        "    def Center_Justify(item):\n",
        "      return item.center(10)\n",
        "\n",
        "\n",
        "    spark.udf.register(\"OrginalAttacks\", Attack_Category, StringType())\n",
        "    spark.udf.register(\"TextJustify\", Center_Justify, StringType())\n",
        "\n",
        "    sql_query = \"\"\"\n",
        "                    SELECT \n",
        "                      TextJustify(service) as service,\n",
        "                      TextJustify(protocol) as protocol,\n",
        "                      OrginalAttacks(label) as new_label,\n",
        "                      COUNT(*) as freq\n",
        "                    FROM df_KDDCup\n",
        "                    WHERE (protocol = 'udp' OR protocol = 'icmp') AND label != 'normal.'\n",
        "                    GROUP BY service, new_label, protocol\n",
        "                    ORDER BY freq DESC\n",
        "              \"\"\"\n",
        "\n",
        "    spark.sql(sql_query).show()\n",
        "\n",
        "    +---------------+---------------+---------+-------+\n",
        "    |        service|       protocol|new_label|   freq|\n",
        "    +---------------+---------------+---------+-------+\n",
        "    |     ecr_i     |      icmp     |      DoS|2808145|\n",
        "    |     eco_i     |      icmp     |    Probe|  12570|\n",
        "    |    private    |      udp      |    Probe|   1688|\n",
        "    |    private    |      udp      |      DoS|    979|\n",
        "    |     other     |      udp      |    Probe|    261|\n",
        "    |     ecr_i     |      icmp     |    Probe|     59|\n",
        "    |    domain_u   |      udp      |    Probe|      9|\n",
        "    |     tim_i     |      icmp     |      DoS|      5|\n",
        "    |     other     |      udp      |      U2R|      3|\n",
        "    |     urp_i     |      icmp     |    Probe|      3|\n",
        "    +---------------+---------------+---------+-------+"
      ],
      "id": "03g8beoOPBMh"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "KDDCup_Analytics_(DF)_solution.ipynb",
      "provenance": []
    }
  }
}