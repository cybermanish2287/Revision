{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distributed Processing Challenges: Handling Data Skew in RDD PySpark**<a href=\"#Distributed-Processing-Challenges:-Handling-Data-Skew-in-RDD-PySpark\" class=\"anchor-link\">¶</a>\n",
    "=================================================================================================================================================================================\n",
    "\n",
    "**`Udemy Course: Best Hands-on Big Data Practices and Use Cases using PySpark`**\n",
    "\n",
    "**`Author: Amin Karami (PhD, FHEA)`**\n",
    "\n",
    "In \\[1\\]:\n",
    "\n",
    "    # Load Spark engine\n",
    "    !pip3 install -q findspark\n",
    "    import findspark\n",
    "    findspark.init()\n",
    "\n",
    "    from pyspark import SparkContext, SparkConf\n",
    "\n",
    "    # Initializing Spark\n",
    "    conf = SparkConf().setAppName(\"Skew\").setMaster(\"local[*]\")\n",
    "    sc = SparkContext(conf=conf)\n",
    "\n",
    "Loading Data Skew<a href=\"#Loading-Data-Skew\" class=\"anchor-link\">¶</a>\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "To understand skew, we create a random data where keys are uniformly\n",
    "distributed.\n",
    "\n",
    "In \\[9\\]:\n",
    "\n",
    "    import numpy as np\n",
    "    import random\n",
    "\n",
    "    key_1 = ['a'] * 10\n",
    "    key_2 = ['b'] * 6000000\n",
    "    key_3 = ['c'] * 800\n",
    "    key_4 = ['d'] * 10000\n",
    "    keys = key_1 + key_2 + key_3 + key_4\n",
    "    random.shuffle(keys)\n",
    "\n",
    "\n",
    "    values_1 = list(np.random.randint(low = 1, high = 100, size = len(key_1)))\n",
    "    values_2 = list(np.random.randint(low = 1, high = 100, size = len(key_2)))\n",
    "    values_3 = list(np.random.randint(low = 1, high = 100, size = len(key_3)))\n",
    "    values_4 = list(np.random.randint(low = 1, high = 100, size = len(key_4)))\n",
    "    values = values_1 + values_2 + values_3 + values_4\n",
    "\n",
    "\n",
    "    pair_skew = list(zip(keys,values))\n",
    "\n",
    "In \\[10\\]:\n",
    "\n",
    "    # load data into RDD\n",
    "    rdd = sc.parallelize(pair_skew, 8)\n",
    "\n",
    "(1) Run a shuffle `groupByKey()` to see how the skew effects computation resources.<a href=\"#(1)-Run-a-shuffle-groupByKey()-to-see-how-the-skew-effects-computation-resources.\" class=\"anchor-link\">¶</a>\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "In \\[2\\]:\n",
    "\n",
    "    data_sample = [(1,4), (2,2), (2,1), (3,5), (2,5), (2,10), (2,7), (3,4), (2,1), (2,4), (4,4)]\n",
    "    rdd_sample = sc.parallelize(data_sample, 3)\n",
    "\n",
    "    rdd_sample.glom().collect()\n",
    "\n",
    "Out\\[2\\]:\n",
    "\n",
    "    [[(1, 4), (2, 2), (2, 1)],\n",
    "     [(3, 5), (2, 5), (2, 10)],\n",
    "     [(2, 7), (3, 4), (2, 1), (2, 4), (4, 4)]]\n",
    "\n",
    "In \\[4\\]:\n",
    "\n",
    "    rdd_sample_grouped = rdd_sample.groupByKey()\n",
    "\n",
    "    # show groupby results\n",
    "    for item in rdd_sample_grouped.collect():\n",
    "        print(item[0], [value for value in item[1]])\n",
    "        \n",
    "    # show partitions:\n",
    "    rdd_sample_grouped.glom().collect()\n",
    "\n",
    "    3 [5, 4]\n",
    "    1 [4]\n",
    "    4 [4]\n",
    "    2 [2, 1, 5, 10, 7, 1, 4]\n",
    "\n",
    "Out\\[4\\]:\n",
    "\n",
    "    [[(3, <pyspark.resultiterable.ResultIterable at 0x7fb5eeb56100>)],\n",
    "     [(1, <pyspark.resultiterable.ResultIterable at 0x7fb5eeb47610>),\n",
    "      (4, <pyspark.resultiterable.ResultIterable at 0x7fb5eeb47be0>)],\n",
    "     [(2, <pyspark.resultiterable.ResultIterable at 0x7fb5eeb47eb0>)]]\n",
    "\n",
    "In \\[11\\]:\n",
    "\n",
    "    grouped_rdd = rdd.groupByKey().cache()\n",
    "\n",
    "    # run a simple data transformation (using map()) on the skewed data\n",
    "    grouped_rdd.map(lambda pair:(pair[0], [(i+10) for i in pair[1]])).count()\n",
    "\n",
    "Out\\[11\\]:\n",
    "\n",
    "    4\n",
    "\n",
    "Mitigate data skewness: SALTING<a href=\"#Mitigate-data-skewness:-SALTING\" class=\"anchor-link\">¶</a>\n",
    "---------------------------------------------------------------------------------------------------\n",
    "\n",
    "In \\[12\\]:\n",
    "\n",
    "    #import random\n",
    "\n",
    "    def salting(val):\n",
    "        tmp = val + \"_\" + str(random.randint(0,5))\n",
    "        return tmp\n",
    "\n",
    "In \\[13\\]:\n",
    "\n",
    "    # salting method:\n",
    "    rdd_salting = rdd.map(lambda x: (salting(x[0]), x[1]))\n",
    "\n",
    "\n",
    "    # actual code\n",
    "    grouped_rdd = rdd_salting.groupByKey().cache()\n",
    "    # run a simple data transformation (using map()) on the skewed data\n",
    "    grouped_rdd.map(lambda pair:(pair[0], [(i+10) for i in pair[1]])).count()\n",
    "\n",
    "Out\\[13\\]:\n",
    "\n",
    "    24\n",
    "\n",
    "(2) Run a shuffle `sortByKey()` to see how the skew effects computation resources.<a href=\"#(2)-Run-a-shuffle-sortByKey()-to-see-how-the-skew-effects-computation-resources.\" class=\"anchor-link\">¶</a>\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "In \\[14\\]:\n",
    "\n",
    "    rdd_sort = rdd.sortByKey(ascending=False, numPartitions=4)\n",
    "    rdd_sort.count()\n",
    "\n",
    "Out\\[14\\]:\n",
    "\n",
    "    6010810\n",
    "\n",
    "Mitigate data skewness: SALTING<a href=\"#Mitigate-data-skewness:-SALTING\" class=\"anchor-link\">¶</a>\n",
    "---------------------------------------------------------------------------------------------------\n",
    "\n",
    "In \\[15\\]:\n",
    "\n",
    "    rdd_sort = rdd_salting.sortByKey(ascending=False, numPartitions=4)\n",
    "    rdd_sort.count()\n",
    "\n",
    "Out\\[15\\]:\n",
    "\n",
    "    6010810\n",
    "\n",
    "(3) Run a shuffle `Join()` to see how the skew effects computation resources.<a href=\"#(3)-Run-a-shuffle-Join()-to-see-how-the-skew-effects-computation-resources.\" class=\"anchor-link\">¶</a>\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "In \\[17\\]:\n",
    "\n",
    "    # example of join\n",
    "\n",
    "    small_rdd1 = sc.parallelize([(2,3), (1,3), (1,4), (3,1), (5,1)], 3)\n",
    "    small_rdd2 = sc.parallelize([(4,3), (0,1), (1,2), (2,1)], 2)\n",
    "\n",
    "\n",
    "    print(small_rdd1.collect())\n",
    "    print(small_rdd2.collect())\n",
    "\n",
    "    [(2, 3), (1, 3), (1, 4), (3, 1), (5, 1)]\n",
    "    [(4, 3), (0, 1), (1, 2), (2, 1)]\n",
    "\n",
    "In \\[18\\]:\n",
    "\n",
    "    join1 = small_rdd1.join(small_rdd2)\n",
    "    join1.collect()\n",
    "\n",
    "Out\\[18\\]:\n",
    "\n",
    "    [(1, (3, 2)), (1, (4, 2)), (2, (3, 1))]\n",
    "\n",
    "In \\[19\\]:\n",
    "\n",
    "    join1.getNumPartitions()\n",
    "\n",
    "Out\\[19\\]:\n",
    "\n",
    "    5\n",
    "\n",
    "In \\[20\\]:\n",
    "\n",
    "    join1.glom().collect()\n",
    "\n",
    "Out\\[20\\]:\n",
    "\n",
    "    [[], [(1, (3, 2)), (1, (4, 2))], [(2, (3, 1))], [], []]\n",
    "\n",
    "In \\[21\\]:\n",
    "\n",
    "    # Generate normal data\n",
    "\n",
    "    key_1 = ['a'] * 5\n",
    "    key_2 = ['b'] * 60\n",
    "    key_4 = ['c'] * 100\n",
    "\n",
    "    keys = key_1 + key_2 + key_4\n",
    "    random.shuffle(keys)\n",
    "\n",
    "\n",
    "    values_1 = list(np.random.randint(low = 1, high = 100, size = len(key_1)))\n",
    "    values_2 = list(np.random.randint(low = 1, high = 100, size = len(key_2)))\n",
    "    values_4 = list(np.random.randint(low = 1, high = 100, size = len(key_4)))\n",
    "    values = values_1 + values_2 + values_4\n",
    "\n",
    "    pair_data = list(zip(keys,values))\n",
    "\n",
    "In \\[22\\]:\n",
    "\n",
    "    small_rdd = sc.parallelize(pair_data, 2)\n",
    "\n",
    "In \\[23\\]:\n",
    "\n",
    "    # Join without salting\n",
    "\n",
    "    rdd_join = rdd.join(small_rdd)\n",
    "    rdd_join.map(lambda x: int(x[1][0] + x[1][1])).reduce(lambda a,b: a+b)\n",
    "\n",
    "Out\\[23\\]:\n",
    "\n",
    "    37848130700\n",
    "\n",
    "Mitigate data skewness: SALTING<a href=\"#Mitigate-data-skewness:-SALTING\" class=\"anchor-link\">¶</a>\n",
    "---------------------------------------------------------------------------------------------------\n",
    "\n",
    "In \\[26\\]:\n",
    "\n",
    "    #import random\n",
    "    # add a random value to the key --> (key, randint)\n",
    "    rdd_new = rdd.map(lambda x: ((x[0], random.randint(0, 10)), x[1])).cache()\n",
    "\n",
    "    # replicate the small data\n",
    "    small_rdd_new = small_rdd.cartesian(sc.parallelize(range(0, 11))).map(lambda x: ((x[0][0], x[1]), x[0][1])).cache()\n",
    "\n",
    "In \\[27\\]:\n",
    "\n",
    "    # Join with salting\n",
    "\n",
    "    rdd_join = rdd_new.join(small_rdd_new)\n",
    "    rdd_join.map(lambda x: int(x[1][0] + x[1][1])).reduce(lambda a,b: a+b)\n",
    "\n",
    "Out\\[27\\]:\n",
    "\n",
    "    37848130700"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
