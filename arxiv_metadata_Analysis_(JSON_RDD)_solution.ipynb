{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6nbzW7E1Ua2"
      },
      "source": [
        "# **Arxiv metadata Analytics with PySpark RDD: JSON case study**<a href=\"#Arxiv-metadata-Analytics-with-PySpark-RDD:-JSON-case-study\" class=\"anchor-link\">¶</a>\n",
        "\n",
        "### Udemy Course: Best Hands-on Big Data Practices and Use Cases using PySpark<a href=\"#Udemy-Course:-Best-Hands-on-Big-Data-Practices-and-Use-Cases-using-PySpark\" class=\"anchor-link\">¶</a>\n",
        "\n",
        "### Author: Amin Karami (PhD, FHEA)<a href=\"#Author:-Amin-Karami-(PhD,-FHEA)\" class=\"anchor-link\">¶</a>\n",
        "\n",
        "In \\[ \\]:\n",
        "\n",
        "    ########## ONLY in Colab ##########\n",
        "    !pip3 install pyspark\n",
        "    ########## ONLY in Colab ##########\n",
        "\n",
        "In \\[ \\]:\n",
        "\n",
        "    ########## ONLY in Ubuntu Machine ##########\n",
        "    # Load Spark engine\n",
        "    !pip3 install -q findspark\n",
        "    import findspark\n",
        "    findspark.init()\n",
        "    ########## ONLY in Ubuntu Machine ##########\n",
        "\n",
        "In \\[ \\]:\n",
        "\n",
        "    from pyspark import SparkContext, SparkConf\n",
        "\n",
        "    # Initializing Spark\n",
        "    conf = SparkConf().setAppName(\"Archive_PySpark\").setMaster(\"local[*]\")\n",
        "    sc = SparkContext(conf=conf)\n",
        "    print(sc)\n",
        "    print(\"Ready to go!\")\n",
        "\n",
        "In \\[ \\]:\n",
        "\n",
        "    # Read and Load Data to Spark\n",
        "    # Data source: https://www.kaggle.com/Cornell-University/arxiv/version/62\n",
        "\n",
        "    import json\n",
        "\n",
        "    rdd_json = sc.textFile(\"data/arxiv-metadata-oai-snapshot.json\", 100)\n",
        "    rdd = rdd_json.map(lambda x: json.loads(x))\n",
        "    rdd.persist()\n",
        "\n",
        "In \\[ \\]:\n",
        "\n",
        "    # Check the number of parallelism and partitions:\n",
        "\n",
        "    print(sc.defaultParallelism)\n",
        "    print(rdd.getNumPartitions())\n",
        "\n",
        "## Question 1: Count elements<a href=\"#Question-1:-Count-elements\" class=\"anchor-link\">¶</a>\n",
        "\n",
        "In \\[ \\]:\n",
        "\n",
        "    # during teaching: https://spark.apache.org/docs/3.0.0-preview/web-ui.html\n",
        "    # http://localhost:4040/\n",
        "\n",
        "    rdd.count()\n",
        "\n",
        "## Question 2: Get the first two records<a href=\"#Question-2:-Get-the-first-two-records\" class=\"anchor-link\">¶</a>\n",
        "\n",
        "In \\[ \\]:\n",
        "\n",
        "    rdd.take(2)\n",
        "\n",
        "## Question 3: Get all attributes<a href=\"#Question-3:-Get-all-attributes\" class=\"anchor-link\">¶</a>\n",
        "\n",
        "In \\[ \\]:\n",
        "\n",
        "    rdd.flatMap(lambda x: x.keys()).distinct().collect()\n",
        "\n",
        "## Question 4: Get the name of the licenses<a href=\"#Question-4:-Get-the-name-of-the-licenses\" class=\"anchor-link\">¶</a>\n",
        "\n",
        "In \\[ \\]:\n",
        "\n",
        "    rdd.map(lambda x: x[\"license\"]).distinct().collect()\n",
        "\n",
        "## Question 5: Get the shortest and the longest titles<a href=\"#Question-5:-Get-the-shortest-and-the-longest-titles\" class=\"anchor-link\">¶</a>\n",
        "\n",
        "In \\[ \\]:\n",
        "\n",
        "    shortest_title_rdd = rdd.map(lambda x: x[\"title\"]).reduce(lambda x, y: x if x < y else y)\n",
        "    longest_title_rdd = rdd.map(lambda x: x[\"title\"]).reduce(lambda x, y: x if x > y else y)\n",
        "\n",
        "    print(\"shortest title: \", shortest_title_rdd)\n",
        "    print(\"longest title: \", longest_title_rdd)\n",
        "\n",
        "## Question 6: Find abbreviations with 5 or more letters in the abstract<a href=\"#Question-6:-Find-abbreviations-with-5-or-more-letters-in-the-abstract\" class=\"anchor-link\">¶</a>\n",
        "\n",
        "In \\[ \\]:\n",
        "\n",
        "    import re\n",
        "\n",
        "    def get_abbrivations(line):\n",
        "        result = re.search(r\"\\(([A-Za-z][^_ /\\\\<>]{5,})\\)\", line)\n",
        "        if result:\n",
        "            return result.group(1) # return 1st match. group (0) will return all the matches\n",
        "\n",
        "In \\[ \\]:\n",
        "\n",
        "    rdd.filter(lambda x: get_abbrivations(x['abstract'])).count()\n",
        "\n",
        "## Question 7: Get the number of archive records per month ('update_date' attribute)<a href=\"#Question-7:-Get-the-number-of-archive-records-per-month-(&#39;update_date&#39;-attribute)\" class=\"anchor-link\">¶</a>\n",
        "\n",
        "In \\[ \\]:\n",
        "\n",
        "    import datetime\n",
        "\n",
        "    def extract_date(DateIn):\n",
        "        d = datetime.datetime.strptime(DateIn, \"%Y-%m-%d\")\n",
        "        return d.month\n",
        "\n",
        "    # check the function:\n",
        "    extract_date('2008-12-13')\n",
        "\n",
        "In \\[ \\]:\n",
        "\n",
        "    rdd.map(lambda x: (extract_date(x[\"update_date\"]),1)).reduceByKey(lambda x,y: x+y).collect()\n",
        "\n",
        "    # sort by values\n",
        "    # rdd.map(lambda x: (extract_date(x[\"update_date\"]),1)).reduceByKey(lambda x,y: x+y).sortBy(lambda l: l[1]).collect()\n",
        "\n",
        "## Question 8: Get the average number of pages<a href=\"#Question-8:-Get-the-average-number-of-pages\" class=\"anchor-link\">¶</a>\n",
        "\n",
        "In \\[ \\]:\n",
        "\n",
        "    import re\n",
        "\n",
        "    def get_Page(line):\n",
        "        search = re.findall('\\d+ pages', line)\n",
        "        if search:\n",
        "            return int(search[0].split(\" \")[0])\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "In \\[ \\]:\n",
        "\n",
        "    rdd_average = rdd.map(lambda x: get_Page(x['comments'] if x['comments'] != None else \"None\"))\n",
        "\n",
        "    # remove 0:\n",
        "    rdd_average = rdd_average.filter(lambda x: x != 0)\n",
        "\n",
        "    average_counter = rdd_average.count()\n",
        "    avarage_summation = rdd_average.reduce(lambda x,y: int(x)+int(y))\n",
        "\n",
        "    print(average_counter)\n",
        "    print(avarage_summation)\n",
        "    print(\"the average of pages is \", avarage_summation/average_counter)"
      ],
      "id": "k6nbzW7E1Ua2"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "arxiv_metadata_Analysis_(JSON_RDD)_solution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  }
}